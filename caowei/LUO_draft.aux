\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Table of contents}{i}{section*.1}}
\@writefile{toc}{\contentsline {chapter}{List of figures}{iii}{section*.3}}
\@writefile{toc}{\contentsline {chapter}{List of tables}{v}{section*.5}}
\citation{maas2011learning}
\citation{ghiassi2013twitter}
\citation{zhang2003question}
\citation{li2002learning}
\citation{wang2012baselines}
\citation{quercia2012tweetlda}
\citation{joachims2002statistical}
\citation{joachims1998text}
\citation{mikolov2010recurrent}
\citation{johnson2014effective}
\citation{socher2013recursive}
\citation{elman1990finding}
\citation{hochreiter1997long}
\citation{li2015constructing}
\citation{breuel2013high}
\citation{sutskever2014sequence}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background and Motivation}{1}{section.1.1}}
\citation{lee2016sequential}
\citation{zhou2015end}
\citation{he2015deep}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Organization of the thesis}{2}{section.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch3}{{2}{5}{Related Work}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Traditional related methods}{5}{section.2.1}}
\citation{vapnik2013nature}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An illustration of feature extraction.\relax }}{6}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{NBde}{{2.1}{6}{An illustration of feature extraction.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Support Vector Machines based methods}{6}{subsection.2.1.1}}
\citation{brucher2002document}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An illustration of SVM for choosing the hypeplane that maximizes the margin in 2D-space.\relax }}{7}{figure.caption.8}}
\newlabel{NBde}{{2.2}{7}{An illustration of SVM for choosing the hypeplane that maximizes the margin in 2D-space.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Naive Bayes based methods:}{8}{subsection.2.1.2}}
\citation{kwon2003text}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces An illustration of k-Nearest Neighbor.\relax }}{9}{figure.caption.9}}
\newlabel{NBde}{{2.3}{9}{An illustration of k-Nearest Neighbor.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}k Nearest Neighbours based methods:}{9}{subsection.2.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces An illustration of Decision trees.\relax }}{10}{figure.caption.10}}
\newlabel{NBde}{{2.4}{10}{An illustration of Decision trees.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Decision Trees based methods:}{10}{subsection.2.1.4}}
\citation{le2014distributed}
\citation{zhang2015character}
\citation{kim2014convolutional}
\citation{johnson2015semi}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces An illustration of simple Neural Netowrks.\relax }}{11}{figure.caption.11}}
\newlabel{NBde}{{2.5}{11}{An illustration of simple Neural Netowrks.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neural Networks related methods}{11}{section.2.2}}
\citation{mikolov2010recurrent}
\citation{xiao2016chinese}
\citation{tang2015document}
\citation{lai2015recurrent}
\citation{wang2016learning}
\citation{wang2016semantic}
\citation{liu2016recurrent}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces An illustration of Recurrent Neural Network.\relax }}{12}{figure.caption.12}}
\newlabel{NBde}{{2.6}{12}{An illustration of Recurrent Neural Network.\relax }{figure.caption.12}{}}
\citation{graves2005framewise}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}A Stacked Residual RNN Model}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The instance of Stacked Residual Bi-LSTM with Word Weight Networks. The left line part is the word weight training module and the right part is the ResNets based module.\relax }}{16}{figure.caption.13}}
\newlabel{NBde}{{3.1}{16}{The instance of Stacked Residual Bi-LSTM with Word Weight Networks. The left line part is the word weight training module and the right part is the ResNets based module.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Word weight based module}{16}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An illustration of Bidirectional LSTM network.\relax }}{17}{figure.caption.14}}
\newlabel{NBde}{{3.2}{17}{An illustration of Bidirectional LSTM network.\relax }{figure.caption.14}{}}
\citation{nair2010rectified}
\citation{he2015deep}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Residual Networks based module}{18}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The left image is a residual block in Residual Networks. The right image is an illustration of Residual Networks.\relax }}{19}{figure.caption.15}}
\newlabel{NBde}{{3.3}{19}{The left image is a residual block in Residual Networks. The right image is an illustration of Residual Networks.\relax }{figure.caption.15}{}}
\citation{graves2013speech}
\citation{chung2015gated}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Stacked structure of Bidirectional LSTM network.\relax }}{20}{figure.caption.16}}
\newlabel{NBde}{{3.4}{20}{Stacked structure of Bidirectional LSTM network.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Stacked Residual Bi-LSTM with Word Weight model}{20}{section.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Work flow of Stacked Residual Bi-LSTM with Word Weight Networks.\relax }}{21}{figure.caption.17}}
\newlabel{NBde}{{3.5}{21}{Work flow of Stacked Residual Bi-LSTM with Word Weight Networks.\relax }{figure.caption.17}{}}
\citation{kingma2014adam}
\citation{socher2013recursive}
\citation{li2002learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experimental Setup}{23}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Datasets}{23}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Sentiment Classification}{23}{subsection.4.1.1}}
\citation{kim2014convolutional}
\citation{kim2014convolutional}
\citation{kalchbrenner2014convolutional}
\citation{socher2013recursive}
\citation{socher2013recursive}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The statistical detail of three datasets in our evaluation.\relax }}{24}{table.caption.19}}
\newlabel{outcome}{{4.1}{24}{The statistical detail of three datasets in our evaluation.\relax }{table.caption.19}{}}
\newlabel{result1}{{4.1}{24}{The statistical detail of three datasets in our evaluation.\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Question type Classification}{24}{subsection.4.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}BaseLines}{24}{section.4.2}}
\citation{socher2013recursive}
\citation{irsoy2014deep}
\citation{liu2016recurrent}
\citation{tai2015improved}
\citation{zhou2015c}
\citation{hinton2012improving}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Some other hyperparameters settings among three datasets.\relax }}{25}{table.caption.22}}
\newlabel{outcome}{{4.2}{25}{Some other hyperparameters settings among three datasets.\relax }{table.caption.22}{}}
\newlabel{result1}{{4.2}{25}{Some other hyperparameters settings among three datasets.\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Hyperparameters and Training}{25}{section.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Results and Analysis}{26}{section.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Text classification}{26}{subsection.4.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Classification accuracy of our method compared with other models on three datasets.\relax }}{27}{table.caption.23}}
\newlabel{outcome}{{4.3}{27}{Classification accuracy of our method compared with other models on three datasets.\relax }{table.caption.23}{}}
\newlabel{result1}{{4.3}{27}{Classification accuracy of our method compared with other models on three datasets.\relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Model Analysis}{28}{subsection.4.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The (a) shows the batch training loss on SST-1 with our method and standard stacked Bi-LSTM. The (b) shows the test accuracy on SST-1 compared with two methods.\relax }}{29}{figure.caption.24}}
\newlabel{NBde}{{4.1}{29}{The (a) shows the batch training loss on SST-1 with our method and standard stacked Bi-LSTM. The (b) shows the test accuracy on SST-1 compared with two methods.\relax }{figure.caption.24}{}}
\bibstyle{IEEEtran}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{31}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibdata{LUOreference}
\bibcite{maas2011learning}{1}
\bibcite{ghiassi2013twitter}{2}
\bibcite{zhang2003question}{3}
\bibcite{li2002learning}{4}
\bibcite{wang2012baselines}{5}
\bibcite{quercia2012tweetlda}{6}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{33}{section*.25}}
\bibcite{joachims2002statistical}{7}
\bibcite{joachims1998text}{8}
\bibcite{mikolov2010recurrent}{9}
\bibcite{johnson2014effective}{10}
\bibcite{socher2013recursive}{11}
\bibcite{elman1990finding}{12}
\bibcite{hochreiter1997long}{13}
\bibcite{li2015constructing}{14}
\bibcite{breuel2013high}{15}
\bibcite{sutskever2014sequence}{16}
\bibcite{lee2016sequential}{17}
\bibcite{zhou2015end}{18}
\bibcite{he2015deep}{19}
\bibcite{vapnik2013nature}{20}
\bibcite{le2014distributed}{21}
\bibcite{zhang2015character}{22}
\bibcite{kim2014convolutional}{23}
\bibcite{johnson2015semi}{24}
\bibcite{xiao2016chinese}{25}
\bibcite{tang2015document}{26}
\bibcite{lai2015recurrent}{27}
\bibcite{wang2016learning}{28}
\bibcite{wang2016semantic}{29}
\bibcite{liu2016recurrent}{30}
\bibcite{graves2005framewise}{31}
\bibcite{nair2010rectified}{32}
\bibcite{graves2013speech}{33}
\bibcite{chung2015gated}{34}
\bibcite{kingma2014adam}{35}
\bibcite{kalchbrenner2014convolutional}{36}
\bibcite{irsoy2014deep}{37}
\bibcite{tai2015improved}{38}
\bibcite{zhou2015c}{39}
\bibcite{hinton2012improving}{40}
