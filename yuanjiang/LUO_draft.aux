\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Table of contents}{i}{section*.1}}
\citation{Lee2015}
\citation{Ronneberger2015}
\@writefile{toc}{\contentsline {chapter}{List of figures}{iii}{section*.3}}
\citation{Lee2015}
\citation{Yang2016}
\citation{Yang2016}
\citation{Lee2015}
\citation{Ronneberger2015}
\citation{Yang2016}
\@writefile{toc}{\contentsline {chapter}{List of tables}{v}{section*.5}}
\citation{Yang2016}
\citation{Zitnick2014}
\citation{Lee2015}
\citation{Quan2016}
\citation{Ronneberger2015}
\citation{Canny1986}
\citation{Sobel2014}
\citation{Dollar2013}
\citation{Shen2015}
\citation{Xie2015}
\citation{Xie2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background and Motivation}{1}{section.1.1}}
\citation{Long2015}
\citation{Yang2016}
\citation{Xie2015}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Object-level boundary detection is different from the traditional local edge detection, where the former mainly focuses on detecting the high-level semantic boundaries. The first image contains the annotations, which are marked with red lines, for object-level boundary detection.\relax }}{2}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{object_boundary_vs_local_edge}{{1.1}{2}{Object-level boundary detection is different from the traditional local edge detection, where the former mainly focuses on detecting the high-level semantic boundaries. The first image contains the annotations, which are marked with red lines, for object-level boundary detection.\relax }{figure.caption.7}{}}
\citation{Lichtman2011}
\citation{Ciresan2012}
\citation{Lee2015}
\citation{Quan2016}
\citation{Zlateski2015}
\citation{Xie2015}
\citation{Canny1986}
\citation{Sobel2014}
\citation{Ciresan2012}
\citation{Lee2015}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Application Scenes}{3}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}EM Images}{3}{subsection.1.2.1}}
\citation{Lee2015}
\citation{Ronneberger2015}
\citation{Arbelaez2011}
\citation{Pinheiro2015}
\citation{Everingham2010}
\citation{Girshick2015}
\citation{Wang2014}
\citation{Shen2014CCPR}
\citation{Shen2016PRL}
\citation{Canny1986}
\citation{Sobel2014}
\citation{Everingham2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Natural Images}{4}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Organization of the thesis}{5}{section.1.3}}
\citation{Canny1986}
\citation{Sobel2014}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{related_work}{{2}{7}{Related Work}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Traditional edge detection}{7}{section.2.1}}
\citation{Malik2001}
\citation{Arbelaez2011}
\citation{Krahenbuhl2015}
\citation{Arbelaez2011}
\citation{Dollar2013}
\citation{Krizhevsky2012}
\citation{Simonyan2014}
\citation{Szegedy2015}
\citation{He2016}
\citation{Krizhevsky2012}
\citation{Girshick2014}
\citation{Girshick2015}
\citation{Ren2015}
\citation{Pinheiro2015}
\citation{Shen2015}
\citation{Kivinen2014}
\citation{Ganin2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Hand-crafted feature based methods}{8}{subsection.2.1.1}}
\citation{Lim2013}
\citation{Shen2015}
\citation{Shen2015}
\citation{Dollar2013}
\citation{Shen2015}
\citation{Long2015}
\citation{Everingham2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Deep learning based methods}{9}{subsection.2.1.2}}
\citation{Xie2015}
\citation{Xie2015}
\citation{Xie2015}
\citation{Arbelaez2011}
\citation{Arbelaez2011}
\citation{Shen2015}
\citation{Xie2015}
\citation{Shen2015}
\citation{Dollar2013}
\citation{Arbelaez2011}
\citation{Yang2016}
\citation{Yang2016}
\citation{Simonyan2014}
\citation{Yang2016}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Object Boundary Detection in Natural Images}{10}{section.2.2}}
\citation{Lichtman2011}
\citation{Helmstaedter2013}
\citation{Sporns2005}
\citation{Helmstaedter2013}
\citation{Laptev2012}
\citation{Kaynig2010}
\citation{Kumar2010}
\citation{Seyedhosseini2011}
\citation{Najman1996}
\citation{Uzunbas2014}
\citation{Zlateski2015}
\citation{Boykov2001}
\citation{Ciresan2012}
\citation{Ronneberger2015}
\citation{Quan2016}
\citation{Fakhry2017}
\citation{Lee2015}
\citation{Carreras2015}
\citation{Ciresan2012}
\citation{Ronneberger2015}
\citation{He2016}
\citation{Quan2016}
\citation{Fakhry2017}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Neuronal Boundary Detection in EM Images}{11}{section.2.3}}
\citation{Lee2015}
\citation{Lee2015}
\citation{Lee2015}
\citation{Lee2015}
\citation{Lee2015}
\citation{Lee2015}
\citation{Ronneberger2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Illustration of VD2D3D, a recursive deep network with the stepwise training.\relax }}{12}{figure.caption.8}}
\newlabel{Lee2015}{{2.1}{12}{Illustration of VD2D3D, a recursive deep network with the stepwise training.\relax }{figure.caption.8}{}}
\citation{Xie2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{methodology}{{3}{15}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Network Structure}{15}{section.3.1}}
\newlabel{network structure}{{3.1}{15}{Network Structure}{section.3.1}{}}
\citation{Xie2015}
\citation{Simonyan2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Three types of cascade-like networks: (a) cascaded network with single-recursive input; (b) cascaded network with multi-recursive inputs; (c) single-stage network with recursive inputs to fine-tune itself.\relax }}{16}{figure.caption.9}}
\newlabel{cascade_structures}{{3.1}{16}{Three types of cascade-like networks: (a) cascaded network with single-recursive input; (b) cascaded network with multi-recursive inputs; (c) single-stage network with recursive inputs to fine-tune itself.\relax }{figure.caption.9}{}}
\citation{Xie2015}
\citation{Xie2015}
\citation{Xie2015}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Training Phase}{17}{section.3.2}}
\newlabel{training phase}{{3.2}{17}{Training Phase}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Illustration of the proposed cascaded fully convolutional network for object boundary detection.\relax }}{18}{figure.caption.10}}
\newlabel{network}{{3.2}{18}{Illustration of the proposed cascaded fully convolutional network for object boundary detection.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Formulation of the Multi-recursive-input}{18}{subsection.3.2.1}}
\citation{Xie2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Loss Function}{19}{subsection.3.2.2}}
\newlabel{fusion of subnet}{{3.4}{20}{Loss Function}{equation.3.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Testing Phase}{21}{section.3.3}}
\newlabel{testing phase}{{3.3}{21}{Testing Phase}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Model Interpretation}{21}{section.3.4}}
\newlabel{interpretation}{{3.4}{21}{Model Interpretation}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Cascaded Architecture \emph  {vs.} Single-stage Architecture}{21}{subsection.3.4.1}}
\citation{Helmstaedter2013}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Multi-recursive-input \emph  {vs.} Single-recursive-input.\relax }}{22}{figure.caption.11}}
\newlabel{multi recursive input}{{3.3}{22}{Multi-recursive-input \emph {vs.} Single-recursive-input.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Multi-recursive-input \emph  {vs.} Single-recursive-input}{23}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}End-to-end Training \emph  {vs.} Stepwise Self-tuning}{23}{subsection.3.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Prediction exmaples from different side-outputs and fused outputs in different stages.\relax }}{24}{figure.caption.12}}
\newlabel{interpretation_png}{{3.4}{24}{Prediction exmaples from different side-outputs and fused outputs in different stages.\relax }{figure.caption.12}{}}
\citation{Lee2015}
\citation{Ronneberger2015}
\citation{Yang2016}
\citation{Zlateski2015}
\citation{Lee2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{25}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{experiments}{{4}{25}{Experiments}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Neuronal Boundary Detection in EM Images}{25}{section.4.1}}
\citation{Shen2016CVPR}
\citation{Lee2015}
\citation{Ronneberger2015}
\citation{Lee2015}
\citation{Ronneberger2015}
\citation{Lee2015}
\citation{Helmstaedter2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Evaluation Metric}{26}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Mouse Piriform Cortex Dataset}{26}{subsection.4.1.2}}
\newlabel{experiment of piriform}{{4.1.2}{26}{Mouse Piriform Cortex Dataset}{subsection.4.1.2}{}}
\citation{Long2015}
\citation{Xie2015}
\citation{Shen2016CVPR}
\citation{Deng2009}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces (b)Neuronal boundary detection and (c) segmentation can be easily converted from each other, which we used in the experiments on Mouse Piriform Cortex Dataset\cite  {Lee2015} and ISBI 2012 EM Segmentation Dataset\cite  {Ronneberger2015}. (1) By applying the graph-based algorithms such as watershed, we can transfer the boundary prediction into segmentation. (2) By calculating the 2D gradient in the segmentation ground-truth, the boundary annotation can be obtained. \relax }}{27}{figure.caption.13}}
\newlabel{segmentation and boundary}{{4.1}{27}{(b)Neuronal boundary detection and (c) segmentation can be easily converted from each other, which we used in the experiments on Mouse Piriform Cortex Dataset\cite {Lee2015} and ISBI 2012 EM Segmentation Dataset\cite {Ronneberger2015}. (1) By applying the graph-based algorithms such as watershed, we can transfer the boundary prediction into segmentation. (2) By calculating the 2D gradient in the segmentation ground-truth, the boundary annotation can be obtained. \relax }{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Mouse Piriform Cortex Dataset\relax }}{27}{table.caption.14}}
\newlabel{piriform_dataset}{{4.1}{27}{Mouse Piriform Cortex Dataset\relax }{table.caption.14}{}}
\citation{Lee2015}
\citation{Lee2015}
\citation{Lee2015}
\citation{Ciresan2012}
\citation{Lee2015}
\citation{Lee2015}
\citation{Lee2015}
\citation{Lee2015}
\citation{Lee2015}
\citation{Beier2016}
\citation{He2016}
\citation{He2016}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Rand F-scores on Mouse Piriform Cortex Dataset\cite  {Lee2015}\relax }}{29}{table.caption.15}}
\newlabel{outcome}{{4.2}{29}{Rand F-scores on Mouse Piriform Cortex Dataset\cite {Lee2015}\relax }{table.caption.15}{}}
\newlabel{piriform fscore}{{4.2}{29}{Rand F-scores on Mouse Piriform Cortex Dataset\cite {Lee2015}\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}ISBI 2012 EM Segmentation Dataset}{29}{subsection.4.1.3}}
\citation{Ronneberger2015}
\citation{Ronneberger2015}
\citation{Ronneberger2015}
\citation{Chen2016}
\citation{Beier2016}
\citation{Quan2016}
\citation{Drozdzal2017}
\citation{Yang2016}
\citation{Yang2016}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Precision (rand merge)-recall (rand split) curves on Mouse Piriform Cortex Dataset\cite  {Lee2015}. Our 3-stage network outperforms all the previous works on this dataset.\relax }}{30}{figure.caption.16}}
\newlabel{piriform prcurve}{{4.2}{30}{Precision (rand merge)-recall (rand split) curves on Mouse Piriform Cortex Dataset\cite {Lee2015}. Our 3-stage network outperforms all the previous works on this dataset.\relax }{figure.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces The Rand F-scores part from the leaderboard of ISBI 2012 EM Segmentation Challenge\cite  {Ronneberger2015}.\relax }}{31}{table.caption.17}}
\newlabel{outcome}{{4.3}{31}{The Rand F-scores part from the leaderboard of ISBI 2012 EM Segmentation Challenge\cite {Ronneberger2015}.\relax }{table.caption.17}{}}
\newlabel{isbi12 fscore}{{4.3}{31}{The Rand F-scores part from the leaderboard of ISBI 2012 EM Segmentation Challenge\cite {Ronneberger2015}.\relax }{table.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Examples selected from PASCAL VOC Contour Dataset\cite  {Yang2016}, where ground-truth contours are labeled as red. There are various objects (human, artificialities, animals, plants, etc.) appearing in complex scenes (indoor and outdoor environments, colorful backgrounds, blurs, textural confusions, etc), which significantly increases the difficulty of detecting the object-level boundary.\relax }}{31}{figure.caption.18}}
\newlabel{voc contour}{{4.3}{31}{Examples selected from PASCAL VOC Contour Dataset\cite {Yang2016}, where ground-truth contours are labeled as red. There are various objects (human, artificialities, animals, plants, etc.) appearing in complex scenes (indoor and outdoor environments, colorful backgrounds, blurs, textural confusions, etc), which significantly increases the difficulty of detecting the object-level boundary.\relax }{figure.caption.18}{}}
\citation{Shen2016CVPR}
\citation{Shen2017TIP}
\citation{Yang2016}
\citation{Shen2016CVPR}
\citation{Yang2016}
\citation{Krahenbuhl2011}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Object Boundary Detection in Natural Images}{32}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Metrics}{32}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}PASCAL VOC Contour Dataset}{32}{subsection.4.2.2}}
\citation{Yang2016}
\citation{Yang2016}
\citation{Long2015}
\citation{Xie2015}
\citation{Yang2016}
\citation{Yang2016}
\citation{Arbelaez2014}
\citation{Dollar2013}
\citation{Xie2015}
\citation{Yang2016}
\citation{Yang2016}
\citation{Yang2016}
\citation{Arbelaez2014}
\citation{Arbelaez2014}
\citation{Dollar2013}
\citation{Xie2015}
\citation{Yang2016}
\citation{Yang2016}
\citation{Yang2016}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Object boundary detection evaluation comparison on PASCAL VOC Contour Dataset\cite  {Yang2016}. Our proposed 3-stage cascaded fully convolutional network achieves the new state-of-the-art on this benchmark, with a significant improvement (around 2\% over the second)).\relax }}{34}{table.caption.19}}
\newlabel{outcome}{{4.4}{34}{Object boundary detection evaluation comparison on PASCAL VOC Contour Dataset\cite {Yang2016}. Our proposed 3-stage cascaded fully convolutional network achieves the new state-of-the-art on this benchmark, with a significant improvement (around 2\% over the second)).\relax }{table.caption.19}{}}
\newlabel{voc fscore}{{4.4}{34}{Object boundary detection evaluation comparison on PASCAL VOC Contour Dataset\cite {Yang2016}. Our proposed 3-stage cascaded fully convolutional network achieves the new state-of-the-art on this benchmark, with a significant improvement (around 2\% over the second)).\relax }{table.caption.19}{}}
\citation{Lee2015}
\citation{Ronneberger2015}
\citation{Xie2015}
\citation{Arbelaez2011}
\citation{Xie2015}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Precision-recall curves on PASCAL VOC Contour Dataset\cite  {Yang2016}.\relax }}{35}{figure.caption.20}}
\newlabel{voc prcurve}{{4.4}{35}{Precision-recall curves on PASCAL VOC Contour Dataset\cite {Yang2016}.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Control Experiments for Model Interpretation}{35}{section.4.3}}
\citation{Xie2015}
\citation{Xie2015}
\citation{Arbelaez2011}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Control Experiment 1: Cascaded Architecture \emph  {vs.} Single-stage Architecture\relax }}{36}{table.caption.21}}
\newlabel{outcome}{{4.5}{36}{Control Experiment 1: Cascaded Architecture \emph {vs.} Single-stage Architecture\relax }{table.caption.21}{}}
\newlabel{control experiment 1}{{4.5}{36}{Control Experiment 1: Cascaded Architecture \emph {vs.} Single-stage Architecture\relax }{table.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Evaluations on Cascaded Architecture \emph  {vs.} Single-stage Architecture}{36}{subsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Qualitative examples to reveal how cascaded networks improve the high-level boundary detection result. For example, the arrows mean the false positive detections are removed step by step.\relax }}{37}{figure.caption.22}}
\newlabel{control 1}{{4.5}{37}{Qualitative examples to reveal how cascaded networks improve the high-level boundary detection result. For example, the arrows mean the false positive detections are removed step by step.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Evaluations on Multi-recursive-input vs. Single-recursive-input}{37}{subsection.4.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Control Experiment 2: Multi-recursive-input vs. Single-recursive-input\relax }}{38}{table.caption.23}}
\newlabel{outcome}{{4.6}{38}{Control Experiment 2: Multi-recursive-input vs. Single-recursive-input\relax }{table.caption.23}{}}
\newlabel{control_experiment_2}{{4.6}{38}{Control Experiment 2: Multi-recursive-input vs. Single-recursive-input\relax }{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Control Experiment 3: End-to-end Training vs. Stepwise Self-tuning\relax }}{38}{table.caption.24}}
\newlabel{outcome}{{4.7}{38}{Control Experiment 3: End-to-end Training vs. Stepwise Self-tuning\relax }{table.caption.24}{}}
\newlabel{control_experiment_3}{{4.7}{38}{Control Experiment 3: End-to-end Training vs. Stepwise Self-tuning\relax }{table.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Evaluations on End-to-end Training vs. Stepwise Self-tuning}{38}{subsection.4.3.3}}
\citation{Liu2017}
\citation{Yang2016}
\bibstyle{IEEEtran}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{41}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{conclusions}{{5}{41}{Conclusions}{chapter.5}{}}
\bibdata{LUOreference}
\bibcite{Yang2016}{1}
\bibcite{Zitnick2014}{2}
\bibcite{Lee2015}{3}
\bibcite{Quan2016}{4}
\bibcite{Ronneberger2015}{5}
\bibcite{Canny1986}{6}
\bibcite{Sobel2014}{7}
\bibcite{Dollar2013}{8}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{43}{section*.25}}
\bibcite{Shen2015}{9}
\bibcite{Xie2015}{10}
\bibcite{Long2015}{11}
\bibcite{Lichtman2011}{12}
\bibcite{Ciresan2012}{13}
\bibcite{Zlateski2015}{14}
\bibcite{Arbelaez2011}{15}
\bibcite{Pinheiro2015}{16}
\bibcite{Everingham2010}{17}
\bibcite{Girshick2015}{18}
\bibcite{Wang2014}{19}
\bibcite{Shen2014CCPR}{20}
\bibcite{Shen2016PRL}{21}
\bibcite{Malik2001}{22}
\bibcite{Krahenbuhl2015}{23}
\bibcite{Krizhevsky2012}{24}
\bibcite{Simonyan2014}{25}
\bibcite{Szegedy2015}{26}
\bibcite{He2016}{27}
\bibcite{Girshick2014}{28}
\bibcite{Ren2015}{29}
\bibcite{Kivinen2014}{30}
\bibcite{Ganin2014}{31}
\bibcite{Lim2013}{32}
\bibcite{Helmstaedter2013}{33}
\bibcite{Sporns2005}{34}
\bibcite{Laptev2012}{35}
\bibcite{Kaynig2010}{36}
\bibcite{Kumar2010}{37}
\bibcite{Seyedhosseini2011}{38}
\bibcite{Najman1996}{39}
\bibcite{Uzunbas2014}{40}
\bibcite{Boykov2001}{41}
\bibcite{Fakhry2017}{42}
\bibcite{Carreras2015}{43}
\bibcite{Shen2016CVPR}{44}
\bibcite{Deng2009}{45}
\bibcite{Beier2016}{46}
\bibcite{Chen2016}{47}
\bibcite{Drozdzal2017}{48}
\bibcite{Shen2017TIP}{49}
\bibcite{Krahenbuhl2011}{50}
\bibcite{Arbelaez2014}{51}
\bibcite{Liu2017}{52}
